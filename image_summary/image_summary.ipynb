{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAVA_EXEC_PATH = \"..\\\\..\\\\llama.cpp\\\\build\\\\bin\\\\Release\\\\llama-llava-cli\"\n",
    "# MODEL_PATH = \"../../Models/ggml_llava-v1.5-7b/ggml-model-f16.gguf\"\n",
    "MODEL_PATH = \"../../Models/ggml_llava-v1.5-7b/ggml-model-f16.gguf\"\n",
    "MMPROJ_PATH = \"../../Models/ggml_llava-v1.5-7b/mmproj-model-f16.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "IMAGE_DIR = Path(DATA_DIR, \"image\")\n",
    "TXT_DIR = Path(DATA_DIR, \"txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "image_paths = sorted(glob.glob(str(IMAGE_DIR.joinpath(\"*.jpg\"))))\n",
    "\n",
    "\n",
    "# image_paths output\n",
    "# [''data/image/anthony-delanoix-Q0-fOL2nqZc-unsplash.jpg'',\n",
    "# ''data/image/arthur-humeau-3xwdarHxHqI-unsplash.jpg'',\n",
    "# ''data/image/bastien-nvs-SprV1eqNrqM-unsplash.jpg'',\n",
    "# ''data/image/marloes-hilckmann-EUzxLX8p8IA-unsplash.jpg'',\n",
    "# ''data/image/michael-fousert-Ql9PCaOhyyE-unsplash.jpg'']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP = 0.1\n",
    "PROMPT = (\n",
    "    \"Scaled Dot-Product Attention.\"\n",
    ")\n",
    "\n",
    "bash_command = f'{LLAVA_EXEC_PATH} -m {MODEL_PATH} --mmproj {MMPROJ_PATH} --temp {TEMP} -p \"{PROMPT}\"'\n",
    "\n",
    "# Bash command output\n",
    "# ~/Code/llama.cpp/build/bin/llava -m ~/Models/ggml_llava-v1.5-7b/ggml-model-q5_k.gguf --mmproj ~/Models/ggml_llava-v1.5-7b/mmproj-model-f16.gguf --temp 0.1 -p \"The image shows a site in Paris. Describe the image like a tourist guide would.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data\\image\\test8.jpg\n",
      "Output:\n",
      "\n",
      "Error:\n",
      "build: 3862 (3f1ae2e3) with MSVC 19.41.34120.0 for x64\n",
      "llama_model_loader: loaded meta data with 18 key-value pairs and 291 tensors from ../../Models/ggml_llava-v1.5-7b/ggml-model-f16.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 12.55 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOG token        = 2 '</s>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size = 12853.02 MiB\n",
      "...................................................................................................\n",
      "key clip.vision.image_grid_pinpoints not found in file\n",
      "key clip.vision.mm_patch_merge_type not found in file\n",
      "key clip.vision.image_crop_resolution not found in file\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "llama_perf_context_print:        load time =   32028.75 ms\n",
      "llama_perf_context_print: prompt eval time =   29172.99 ms /   624 tokens (   46.75 ms per token,    21.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =   32142.50 ms /    96 runs   (  334.82 ms per token,     2.99 tokens per second)\n",
      "llama_perf_context_print:       total time =   64504.38 ms /   720 tokens\n",
      "\n",
      "Return code: 0. Finished in 65.76 sec\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "for image_path in image_paths:\n",
    "    print(f\"Processing {image_path}\")\n",
    "    image_name = Path(image_path).stem\n",
    "    image_summary_path = TXT_DIR.joinpath(image_name + \".txt\")\n",
    "\n",
    "    # add input image and output txt filenames to bash command\n",
    "    bash_command_cur = f'{bash_command} --image \"{image_path}\" > \"{image_summary_path}\"'\n",
    "\n",
    "    # run the bash command\n",
    "    time_start = time.time()\n",
    "    process = subprocess.Popen(\n",
    "        bash_command_cur, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
    "    )\n",
    "\n",
    "    # get the output and error from the command\n",
    "    output, error = process.communicate()\n",
    "\n",
    "    # commment output and error for less verbose output\n",
    "    print(\"Output:\")\n",
    "    print(output.decode(\"utf-8\"))\n",
    "\n",
    "    print(\"Error:\")\n",
    "    print(error.decode(\"utf-8\"))\n",
    "\n",
    "    # return the code of the command\n",
    "    return_code = process.returncode\n",
    "    time_end = time.time()\n",
    "    exec_time_sec = time_end - time_start\n",
    "\n",
    "    print(f\"Return code: {return_code}. Finished in {exec_time_sec:.2f} sec\")\n",
    "    print()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = sorted(glob.glob(str(TXT_DIR.joinpath(\"*.txt\"))))\n",
    "image_texts = []\n",
    "\n",
    "for filepath in filepaths:\n",
    "    with open(filepath, \"r\") as f:\n",
    "        image_text = f.read()\n",
    "    image_texts.append(image_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip_model_load: model name:   openai/clip-vit-large-patch14-336\n",
      "clip_model_load: description:  image encoder for LLaVA\n",
      "clip_model_load: GGUF version: 2\n",
      "clip_model_load: alignment:    32\n",
      "clip_model_load: n_tensors:    377\n",
      "clip_model_load: n_kv:         18\n",
      "clip_model_load: ftype:        f16\n",
      "\n",
      "clip_model_load: loaded meta data with 18 key-value pairs and 377 tensors from ../../Models/ggml_llava-v1.5-7b/mmproj-model-f16.gguf\n",
      "clip_model_load: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "clip_model_load: - kv   0:                       general.architecture str              = clip\n",
      "clip_model_load: - kv   1:                      clip.has_text_encoder bool             = false\n",
      "clip_model_load: - kv   2:                    clip.has_vision_encoder bool             = true\n",
      "clip_model_load: - kv   3:                   clip.has_llava_projector bool             = true\n",
      "clip_model_load: - kv   4:                          general.file_type u32              = 1\n",
      "clip_model_load: - kv   5:                               general.name str              = openai/clip-vit-large-patch14-336\n",
      "clip_model_load: - kv   6:                        general.description str              = image encoder for LLaVA\n",
      "clip_model_load: - kv   7:                     clip.vision.image_size u32              = 336\n",
      "clip_model_load: - kv   8:                     clip.vision.patch_size u32              = 14\n",
      "clip_model_load: - kv   9:               clip.vision.embedding_length u32              = 1024\n",
      "clip_model_load: - kv  10:            clip.vision.feed_forward_length u32              = 4096\n",
      "clip_model_load: - kv  11:                 clip.vision.projection_dim u32              = 768\n",
      "clip_model_load: - kv  12:           clip.vision.attention.head_count u32              = 16\n",
      "clip_model_load: - kv  13:   clip.vision.attention.layer_norm_epsilon f32              = 0.000010\n",
      "clip_model_load: - kv  14:                    clip.vision.block_count u32              = 23\n",
      "clip_model_load: - kv  15:                     clip.vision.image_mean arr[f32,3]       = [0.481455, 0.457828, 0.408211]\n",
      "clip_model_load: - kv  16:                      clip.vision.image_std arr[f32,3]       = [0.268630, 0.261303, 0.275777]\n",
      "clip_model_load: - kv  17:                              clip.use_gelu bool             = false\n",
      "clip_model_load: - type  f32:  235 tensors\n",
      "clip_model_load: - type  f16:  142 tensors\n",
      "clip_model_load: CLIP using CPU backend\n",
      "clip_model_load: text_encoder:   0\n",
      "clip_model_load: vision_encoder: 1\n",
      "clip_model_load: llava_projector:  1\n",
      "clip_model_load: minicpmv_projector:  0\n",
      "clip_model_load: model size:     595.49 MB\n",
      "clip_model_load: metadata size:  0.13 MB\n",
      "clip_model_load: params backend buffer size =  595.49 MB (377 tensors)\n",
      "clip_model_load: compute allocated memory: 32.89 MB\n",
      "encode_image_with_clip: image embedding created: 576 tokens\n",
      "\n",
      "encode_image_with_clip: image encoded in  5705.97 ms by CLIP (    9.91 ms per image patch)\n",
      "\n",
      " The image showcases a beautiful view of the Eiffel Tower in Paris, France. The iconic landmark stands tall and proudly in the sky, towering over the city. The scene is bustling with activity, as numerous cars and trucks are scattered throughout the area, likely indicating a busy day in the city. The presence of multiple people in the scene suggests that this is a popular tourist destination, with visitors enjoying the view and exploring the city. The combination of the Eiffel Tower, the busy streets, and the lively atmosphere make this an unforgettable sight for anyone visiting Paris.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(image_texts[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
